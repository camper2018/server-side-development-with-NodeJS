JavaScript, when it is first designed was meant to be a scripting language to be used within the browser.
Now JavaScript has gone way beyond its original intention, and is being used for writing applications, both to be run using frameworks within the browser,
and also to run applications on the server side.
JavaScript originally was never designed with any common libraries.
If you look at standard programming languages like C, C++, Java, and so on,
they all have standard libraries that enable you to access the underlying hardware.
And also provide a structured way of organization the application into multiple files and then combining them together when you create an application.
JavaScript never had any of this support when it began.
But of course, people understood the difficulties when you need to expand JavaScript beyond a single file which is used as a scripting language for the browser.
Now, if you have a very large JavaScript application, it becomes cumbersome to write the entire code in one single file.
And obviously you want the results to be able to break your application into multiple facts.
Unlike traditional programming languages, JavaScript never had a way of distributing the code into multiple files and then combining them together.
So this is where the CommonJS API came in to fill in this gap that fills in the needs for some common application.
And this CommonJS format defines a module format that can be used for breaking up your JavaScript application into multiple files.
Node adopts that CommonJS format for organizing our JavaScript application into multiple files.
And within JavaScript, with the CommonJS format, each file becomes its own Node module.
So within that file, the CommonJS specification provides a variable called the module variable which is a JavaScript object.
And this gives you access to the current module definition within a file.
And on this module object, you have the module.exports property which determines the export from the current module.
So when you assign something to the module.exports property, then that becomes the exported value from the current module.
So that when this module is imported into another file of our Node application, then whatever is exported from this module becomes available in the second application.
When you need to import a module into another module, this is where the require function is used to import the module.
Node modules can be of three kinds:
1. We have file-based Node modules where we define the Node module within a file, within our application and we make use of it within our application.
2. We have core Node modules that are already part of Node.
The Node designers kept these core modules intentionally small so that Node can be kept small.
And also provide sufficient functionality so that external module designers can add in their own functionality that can be leveraged when we developed Node applications.
So the core modules include path, file system (fs), os, util, and a few others.
3. Then we have external Node modules.
These are third-party Node modules that are developed by Node developers, and then made available through the Node ecosystem.
So these external Node modules can be install within our system using NPM.

Package.json is a Menifest file for our app. It is a file that contains information about the files in our app i.e metadata eg: name, version, liscence, author, modules used etc
With node, we have the option of using JavaScript or Typescript as a language.
The documentation for node and also express, all use standard JavaScript as a default. So we will be using standard JavaScript in this course or ES2015 JavaScript in this course.
If we prefer to use TypeScript, we can set up our node examples to write our application in TypeScript.
But then we will have to transpile that code into JavaScript so that it can be run using Node.
So we have to set up additional infrastructure in order to be able to use TypeScript.
Asynchronous execution and Callbacks:
Before we proceed on to talk about Node Modules and Callbacks, we need to understand two salient features about the JavaScript language itself.
First and foremost, JavaScript supports what is called as first-class functions.
Is that a function can be treated just like any other variable.
And hence, functions can be passed around as parameters inside function calls to other functions.
And that essentially allows us to send in functions as callback functions that can be called from another Node module in order to get some work accomplished.
Then look at how this is very useful in supporting callbacks in Node.js
The second aspect about JavaScript is the support for Closures.
A function defined inside another function automatically gets access to the variables that are declared in the outer function. So even if the outer function is completed execution, when the inner function executes later the inner function will still have access to the values of the variables within that outer function.
And this is again, very effectively used when we use Callbacks in Node applications.

HTTP VERBS:
Http is a client-server communication protocol.
Get, Post, Put, Delete, Head, Options, Trace, Connect.

Intro To Express:
The Node designers intentionally kept node small with a small number of code modules so that they can leave it up to third party developers to come up with innovative solutions to problems.
One of the most popular third party Node modules or frameworks for building HTTP servers is Express.
Express is a fast, unopinionated, minimalist framework that runs on top of Node.js and supports Web development.
Express also provides a way of extending and adding functionality to Express through third-party middleware.
To use Express in your project, of course, the first step is to install Express and since Express is a Node module, we install it by saying npm install express --save and this would install Express into your local project.
The middleware that Express supports provide a lot of plug-in functionality that would be used to enhance your Express application,
plug-in functionality like for example we will look at a middleware called Morgan which allows you to print out log information to the screen about the requests that come into your server.
Similarly, we'll look at another middleware called BodyParser, which allows you to parse the body of the incoming HTTP request message and extract information from it for use within your Express application.
Similarly, they can serve up static Web resources from our server using the express.static so this will serve up information from a folder within our Express project,
and in declaring the project we can say __filename and __dirname which gives you the full path for the file or the directory for the current module.
Brief Tour of a Node Module:
Examine a Package.json file and look at semantic versioning.
So when you specify the version of the package that you use, you always specify the version by specifying the Major Version.Minor Version.the patch.
So when you install a package, it is always identified by these three numbers:
major version which might introduce breaking changes so which means that if you are installing a newer version of package it may not be completely backward compatible with previous versions.
It may introduce breaking changes whereby you may need to go back and fix the code that you might have written in the earlier version of your project.
The minor version introduces some minor changes to your package and may not be breaking changes.
A patch would be a bug fix that is often issued when a small bug is discovered.
So patches usually do not lead to any breaking changes and so you can easily use a higher version or a higher patch version of a particular package that you're using within your Node application.
With npm install, we can specifiy the acceptable package version as follows:
1.Exact: means exact version match : npm install express@4.0.0
2.Patch acceptable: npm install express@"~4.0.0"
3.Minor version acceptable:npm install express@"^4.0.0"
This kind of information is also saved in the package.json file.
The third-party modules and their version info is stored in package.json file in as dependencies.

Package.lock.json file:
This is being installed by the newer versions of npm.
The package-lock.json file is automatically generated by npm which stores information about the exact tree that was generated when you install other Node modules
and this is very useful when you need to do installation of the Node modules at another location.
So for example if you download a Git repository and try to recreate this project on another computer, you would simply type npm install on the prompt and that'll prompt your Node application to automatically install everything that is specified in the dependencies here for you.
While creating that the package-lock.json stores additional information that is used by npm to do the correct installation of all the npm modules that are required.

Exercise (Instructions): Introduction to Express:
In this exercise you learnt to use the Express framework to design and implement a web server.
a.A Simple Server using Express:
1.Create a folder named node-express in the NodeJS folder and move to that folder.
2.Copy the public folder from node-http to this folder.
3.At the prompt, type the following to initialize a package.json file in the node-express folder:
npm init
4.Accept the standard defaults suggested until you end up with a package.json file as we have in folder.
5.Then, install the Express framework in the folder by typing the following at the prompt:
  npm install express@4.16.3 --save
6.Create a file named .gitignore and add the following to it:
  node_modules
7.Create a file named index.js and add the code to it as seen in commit "Express Example".
8.Start the server by typing the following at the prompt, and then interact with the server:
  npm start
9.Initialize a Git repository, add the files and do a Git commit with the message "Express Example".

b.Serving Static Files:
1.Install morgan by typing the following at the prompt. Morgan is used for logging purposes:
  npm install morgan@1.9.0 --save
2.Update index.js to include it.
3.Start the server and interact with it and observe the behavior.
4.Do a Git commit with the message "Express Serve Static Files".

REST:
Web Services are a way of designing systems to support interoperability among systems that are connected over a network like the internet as we see today.
This is what we refer to as a service-oriented architecture.
Now, what this means is that you provide a standardized way for two machines that are connected to the internet to be able to communicate with each other at the application layer level for web-based applications using open standards.
Two common approaches that are used for supporting web services are:
1. SOAP: The Simple Object Access Protocol based web services which uses the web services description language for specifying how the two endpoints communicate with each other.
Typically SOAP is based on using XML as the format for the messages being exchanged between the two endpoints.
2.Representational State Transfer or REST also uses web standards, but the exchange of data between the two endpoints could be either XML or increasingly using JSON as the format.
The REST way of interoperability is simpler compared to SOAP and hence, REST has found a lot wider deployment in the web services world.
Typically, client-server communication is facilitated using REST where the server supports a REST API and the client can then invoke the REST API endpoints in order to obtain information or to upload information to the server.
Within Representational State Transfer, there are four basic design principles:
1. First and foremost, REST is built upon HTTP protocol, so it uses all the HTTP methods.
2. Second, REST is designed to be stateless, meaning that the server doesn't store any information about the state after the communication is completed.
3. Third, the REST way of providing resources is to expose a directory structure like URLs (Uniform Resource Locators - URLs).
4. Fourth, the format for data exchange is typically JSON or XML or both can be supported using REST.
In the REST world you often hear people talking about nouns, verbs, and representations.
1. Nouns specifically refer to resources and these resources are usually identified by their URLs and these are unconstrained.
  Now, these resources can be organized into a hierarchy of the specification of this URI.
  So, as we traverse the path, we go from the more general to the more specific of the resource.
  This directory structure enables you to identify the resources that you use or provide from your server-side very easily.
2. Verbs are constraint and these specify the actions to be done on the resources.
   GET, POST, PUT and DELETE, are mapped into the four CRUD operations that we can carry out on a database that stores these resources on the server-side,
   the READ, CREATE, UPDATE, and DELETE operations.
3. Representsation is, when the information needs to be conveyed from the server to the client or from the client to the server, how you encode the information.
  Typically, either using JSON or using XML.

DELETE operation would be idempotent because if you try to delete a resource and the resource exists, it will be deleted.
If you are trying to delete a non-existing resource, it won't cause any further modification to the server-side, except that the server will reply with an error saying that the resource doesn't exist.
Similarly, the GET operation is also an idempotent operation because it is not making any modifications to the resource on the server-side.
POST and PUT of course are going to modify the resource on the server-side, either create a new resource or modify an existing resource on the server-side.
Each endpoint is identified by a URI, and we can specify the various operations to be done on each endpoint using the appropriate HTTP verb, the GET, PUT, POST, or DELETE.
The last part that we need to emphasize is that server-side should be completely stateless,
which means that the server-side does not track the client state because if the server needs to track the clients state, it will not be scalable.
So, for a scalable implementation of the server-side, you normally use a stateless server on the server-side.
So, every request that the client sends to the server will be treated as an independent request and will not reflect upon previous requests that have already been handled by the server from that particular client.
So, it's the responsibility of the client to track its own state, either in the form of using cookies or using a client-side database, whatever means that is suitable.
Now, this approach where the client tracks its own state is a lot more scalable because each individual client will be responsible for tracking its own state.
This is where the client-side MVC setup helps us in this regard.

EXPRESS ROUTER:
If you are implementing an Express application which supports multiple REST API endpoints,
then it makes sense to subdivide the code into multiple modules and then make use of them to construct the overall Express application.
So this where the Express router comes to our aid.
Express router enables us to subdivide our application and organize it into multiple mini Express-like applications, which combine together to form the Express application.
An Express router defines many Express application, and within that many Express application,
you can, for example, deal with one particular REST API endpoint in more detail, or one particular pattern of REST API endpoint in more detail.

Exercise (Instructions): Express Router:
In this exercise, we used the Express framework and Express router to build a server supporting a REST API.

Setting up a REST API:
1.You will continue in the node-express folder and modify the server in this exercise.
2.Install body-parser by typing the following at the command prompt:
     npm install body-parser@1.18.3 --save
3.Update index.js to add /dishes and /dishes:dishId REST api endpoints.
4.Start the server and interact with it from the browser/postman.
5.Do a Git commit with the message "Express Simple REST Api".

Using Express Router:
1.Create a new folder named routes in the node-express folder.
2.Create a new file named dishRouter.js in the routes folder and add the code to implement /dishes route.
3.Update index.js to use dishesRouter module within the main express app.
4.Start the server and interact with it and see the result.
5.Do a Git commit with the message "Express Router".

Assignment 1:
In this assignment we will continue the exploration of Node modules, Express and the REST API.
We will implement /dishes/:dishId endpoint in the dishRouter.
We will design two new express routers to support REST API end points for promotions and leadership.

Step-By-Step Assignment Instructions:

Assignment Overview:
At the end of this assignment, you should have completed the following tasks to update the server:

1. Created a Node module using Express router to support the routes for the dishes REST API.
2. Created a Node module using Express router to support the routes for the promotions REST API.
3. Created a Node module using Express router to support the routes for the leaders REST API.

Assignment Requirements:
The REST API for our React application that we built in the previous courses requires us to support the following REST API end points:

http://localhost:3000/dishes/:dishId
http://localhost:3000/promotions and http://localhost:3000/promotions/:promoId
http://localhost:3000/leaders and http://localhost:3000/leaders/:leaderId
We need to support GET, PUT, POST and DELETE operations on each of the endpoints mentioned above, including supporting the use of route parameters to identify a specific promotion and leader.
We have already constructed the REST API for the dishes route in the previous exercise.
This assignment requires you to complete the following three tasks. Detailed instructions for each task are given below.

Task 1
In this task you will create a separate Node module implementing an Express router to support the REST API for the dishes.
You can reuse all the code that you implemented in the previous exercise. To do this, you need to complete the following:
Update the Node module named dishRouter.js to implements the Express router for the /dishes/:dishId REST API end point.

Task 2
In this task you will create a separate Node module implementing an Express router to support the REST API for the promotions.
To do this, you need to complete the following:
Create a Node module named promoRouter.js that implements the Express router for the /promotions and /promotions/:promoId REST API end points.
Require the Node module you create above within your Express application and mount it on the /promotions route.

Task 3
In this task you will create a separate Node module implementing an Express router to support the REST API for the leaders.
To do this, you need to complete the following:

Create a Node module named leaderRouter.js that implements the Express router for the /leaders  and /leaders/:leaderId REST API end points.
Require the Node module you create above within your Express application and mount it on the /leaders route.

Express Generator:
Express Generator is a quick scaffolding tool that will help us to quickly build up the structure for an Express application with some starting code already built and some standard middleware already included into the application.
And so all that we need to do is install the Express Generator via Command line interface as a global NPM module, and then use that to scaffold out our Express application.
sudo npm install express-generator -g
moved into the node-express folder
Then type express <app-name> i.e express conFusionServer
And this will generate a folder with the name of the application that you have typed in.
There are various options available for you to generate your Express application.
It can use different kinds of view generators like jade, EJS, and so on.
In this course, we will be using Express purely as a server that supports REST API.
And once you scaffold out your Express application, you just move into the application folder
and do an npm install to install all the preconfigured modules that are already included in your default Express application.

Exercise (Instructions): Express Generator:
Objectives and Outcomes
In this exercise, you will use the Express generator to scaffold out an Express application. Thereafter you will modify the application to support REST API making use of the Node modules that you developed as part of the first assignment. At the end of this exercise, you will be able to:

Generating an Express application using the express-generator
Modify the Express application to support the REST API by adding routes

Installing express-generator
1.Install express-generator by typing the following at the prompt:
  sudo npm install express-generator@4.16.0 -g
2.To scaffold out an Express application, type the following at the prompt:
  express conFusionServer
3.Next, move to the conFusionServer folder. Type the following at the command prompt to install all the Node modules:
  npm install
4.You can start the Express server by typing the following at the prompt:
  npm start
5.Add a file named .gitignore to the project folder and type the following into the file:
  node_modules
6.Initialize a Git repository and do a Git commit with the message "Express Generator".

Implementing a REST API
1. Now, copy the dishRouter.js, promoRouter.js and leaderRouter.js from your first assignment (node-express/routes folder) to the routes folder within the Express application that you just scaffolded out.
2. Furthermore, copy the index.html and aboutus.html file from the node-express/public folder to the public folder in your new project.
3. Then, open the app.js file and then update the code in there
4. Save the changes and run the server. You can then test the server by sending requests and observing the behavior.
5. Do a Git commit with the message "Express Generator REST API".

MongoDB:
The NoSQL databases themselves can be classified into four different categories:
1.document based databases like MongoDB
2. key value based databases like Redis
3. column-family-based databases like Cassandra
4. graph databases like Neo4J
A document is a self-contained unit of information
and can be in many different formats,
JSON being one of the most popular formats for storing documents in a document database.
Documents themselves can be organized into collections.
So a collection is a group of documents and in turn, the database itself can be considered as a set of collections.
The document in MongoDB is nothing but a JSON document.
In fact, MongoDB stores the document in a more compact form called as the BSON format or binary JSON format.
Every document in MongoDB database must have an ID field, an _id field, which acts as the primary key for the document.
And this field is unique for each document.
The ID field itself can be used in many formats and one particular format that MongoDB automatically assigns in case you don't choose to use your own ID field is the object ID that is created by default by MongoDB.
The object ID field itself is a 12 byte field.
The first four bytes includes a timestamp, the typical Unix timestamp in the resolution of a second.
Then the next three bytes towards the machine ID, the machine on which the Mongo server is running.
and the next two bytes is the process ID, the specific Mongo process which has created this document
and then the last field is an increment(self-incrementing).
When given an ID, you can easily retrieve information from this ID.
For example, you can get hold of the ObjectID and then call the getTimestamp method of the object ID
and this will return the timestamp in the ISO date format.
So that will enable you to identify when this document has been created.

Exercise (Instructions): Introduction to MongoDB:
Objectives and Outcomes
a.Download and Installing MongoDB
b.Start the server and interact with it using the Mongo REPL shell

1.Go to http://www.mongodb.org, then download and install MongoDB as per the instructions given there.
2.Create a folder named mongodb on your computer and create a subfolder under it named data.
3.Move to the mongodb folder and then start the MongoDB server by typing the following at the prompt:
   mongod --dbpath=data --bind_ip 127.0.0.1
   We created q data subfolder inside the mongodb folder.
   So the dbpath here essentially takes the path to the folder, which will store the data for my Mongo application.
   We can execute this mongod command from any location on our computer ss long as we specify the complete path to the location of the data folder where our MongoDB data is going to be stored.
   Since we are running mongod server already in the mongodb folder, and the data folder is a sub-folder of the mongodb folder,
   we can just simply execute the command by saying mongodb -- dbpath=data.

4.Open another command window and then type the following at the command prompt to start the mongo REPL shell:
   mongo
5.The Mongo REPL shell(read evaluate print loop) will start running and give you a prompt to issue commands to the MongoDB server. At the Mongo REPL prompt, type the following commands one by one and see the resulting behavior:
  db
  use conFusion
  db
  db.help()
6.You will now create a collection named dishes, and insert a new dish document in the collection:
  db.dishes.insert({ name: "Uthappizza", description: "Test" });
7.Then to print out the dishes in the collection, type:
  db.dishes.find().pretty();
8.Next, we will learn the information encoded into the ObjectId by typing the following at the prompt:
  var id = new ObjectId();
  id.getTimestamp();
9.Type "exit" at the REPL prompt to exit the Mongo REPL.

Node and MongoDB:
Node MongoDB Driver module provides a high-level API that enables us to access the Mongo server from within our Node application.
It provides you with many methods that enable you to interact with your Mongo server.
It allows you to perform various database operations like inserting, deleting and updating an existing record or adding new records to your database.
Also, it provides various ways of querying the documents that are already within the database.
The driver itself supports both callback based and promise based interactions with the MongoDB server.

Exercise (Instructions): Node and MongoDB Part 1:
Objectives and Outcomes:
In this exercise you will install the Node MongoDB driver module and configure your Node application to communicate with the MongoDB server. At the end of this exercise, you will be able to:

Install and use the Node MongoDB driver
Interact with the MongoDB database from a Node application

Installing the Node MongoDB Driver Module
1.Create a new folder named node-mongo and move into the folder.
2.At the prompt, type the following to initialize a package.json file in the node-mongo folder:
  npm init
3.Accept the standard defaults suggested until you end up with a package.json file.
4.Install the Node MongoDB driver and the Assert module by typing the following at the prompt:
  npm install mongodb@3.0.10 --save
  npm install assert@1.4.1 --save

A Simple Node-MongoDB Application
1.Create a new file named index.js and add code in it.
2.Make sure that your MongoDB server is up and running
3.Type the following at the prompt to start the server and see the result.
  npm start
4.Add a .gitignore file with the contents "node_modules" in the project folder.
5.Initialize the Git repository, check in all the files and do a Git commit with the message "Node MongoDB Example 1".

Exercise (Instructions): Node and MongoDB Part 2
Objectives and Outcomes
In this exercise you will continue to explore communicating from our Node application to the MongoDB server.
At the end of this exercise you will be able to:
1.Develop your own Node module (file based module) containing some common MongoDB operations
2.Use the Node module in your application and communicate with the MongoDB server

Implementing a Node Module of Database Operations:
1.Create a new file named operations.js that contains a few MongoDB operations and add the code

Using the Node Module for Database Operations
1.Update the file named index.js
2/Run the server by typing the following at the prompt and observe the results:
  npm start
3.Do a Git commit with the message "Node MongoDB Example 2".
Conclusions
In this exercise you created a Node module to package some database operations,
and then used the module to interact with the MongoDB server.

Callback Hell and Promises:
Heavily nested callback code, causes the Callback Hell problem and it results from our tendency to write programs top-down.
We are still hung up with our sequential way of writing code and so we see it more convenient to write code top to bottom, and look at it as if it is executing in that order.
Now we can work around the Callback Hell problem by not using anonymous functions for the callbacks but instead, declaring those functions with specific names, and then avoid the way we write the code as you saw here.
That is one of the approaches that people take to deal with the Callback Hell problem.
Another approach that is used to deal with for the Callback Hell problem,is the use of promises.

A promise is a mechanism that supports asynchronous computation.
So if you have amount of work that needs to be done, the promise acts as a proxy for a value which is not known at the moment but the promise is given to you.
But when the value becomes available, it will be available in the future.
So the promise represents a placeholder for that value.
If the value results correctly, then your promise results correctly and you can have a piece of code executed in order to handle the fact that the promise resolved correctly,
if not then you handle the error in that situation.
So, a promise will resolve either into resolve or the rejection of the promise.

The node MongoDB driver natively supports promises.
So, if you do not specify a callback, the calls to their functions will return promises.
So, we're going to update our application to make use of promises to avoid the callback hell issue
Exercise (Instructions): Callback Hell and Promises:
Objectives and Outcomes
In this exercise you will learn to use promise support to update the Node application to avoid callback hell.
Using Promises
1.Update operations.js
2.Next open index.js and update it
3.Run the node application and see the result
4.Do a Git commit with the message "Node Callback Hell and Promises".

Mongoose ODM
We learned how the node MongoDB driver enables our node application to communicate with a MongoDB server.
When the documents are stored in the database, the MongoDB driver itself imposes no structure on the documents.
Any document can be stored in any collection.We can easily insert documents that don't necessarily comply with the structure.
MongoDB relies on the developer to enforce the structure on the documents,
and gives the complete responsibility to the developer to make sure that documents of the correct structure are added and maintained in the various collections.
If we are very particular that the structure of the documents in a collection always have a specified structure, and always will have the specific set of fields,
then the MongoDB itself doesn't impose that neither does the node MongoDB driver that we have seen in the previous lesson.
his is where we will need a more formal way of imposing structure on the documents that are stored in a collection in a MongoDB database.
his is where the Mongoose node module comes to our help.
The Mongoose node module imposes a standardized structure for the documents that are stored in a particular collection.
So, that is why we often hear people referring to this as the Mongoose ODM.
The ODM itself is interpreted by some people to mean Object Data Model or sometimes referred to as Object Document Mapping, or some people refer to it as ORM or Object Relational Mapping.
Now, when we talk about relational that applies a lot more to relational databases,
but then with SQL databases we needed explicitly the object to relational mapping to be put in between the database and our application itself.
Because within the application we would be looking at objects but their storage in an SQL database will be in the form of records, and so you need an explicit mapping.
As we saw with the NoSQL database, this was not explicitly required.
But if we need to impose structure on our documents that are stored in a collection then the use of Mongoose to impose this structure is very, very useful.
The way Mongoose goes around imposing structure on the documents is through the use of schema.
Mongoose schema, implies a structure on the data that is stored in a document in your database.
So, it defines all the fields of our document, and also specifies the types of the fields,
and also can provide us with additional features that can enable validation on these fields.
So, for example, the various schema types that are supported in Mongoose include: String, Number, Date, Buffer, Boolean, Mixed, object ID, and Array.
An array schema type would allow us to create an array of sub-documents inside the document.
Once you define a schema, the schema is then used in Mongoose to create a model function, and that is what enables you to define the structure for your documents in the database.
Schemas themselves can be nested to enable supporting embedded or subdocuments.
The sub-documents typically are accommodated either through specifying an additional schema,
and then defining one of the fields of the schema to be off the type of the other schema.
Or you can even go with an array of another schema type within a second schema that you define.
For example, if you define a schema earlier, so for example, you previously defined a commentSchema, now you can also define the comment field into another schema (e.g dishSchema) and then specify that that field will be of the type of the previous schema that you have defined i.e {coment: [commentSchema]} type.
So, an array of comments that will be included in each dish document, thereby, you can have more than one comment sub-document enclosed inside a dish document.
You can also define additinal field in your schema that is : timestamps: true
The timestamps allow you to have two different fields in the document: the created_at field and the updated_at fields, both of which are timestamps stored in the form of an ISO date string in the document.
Now, once we define the schema, to make use of this in our application, we need to create a model from that schema that we have just defined.
e.g var Dishes = mongoose.model('Dish', dishSchema);
we will define a Mongoose model and specify that the model is off the type dishSchema in this example.
when you give a name to the model here, we are specifying the name as Dish.
Now, when we use this dish model in our node application where we are making use of Mongoose, then this will be transformed and mapped into a collection in my MongoDB database i.e dishes
So, Mongoose automatically knows that when you specify a name, it'll automatically construct the plural of that name and then give the collection the name, which is the plural of the model name that you specify.
Mongoose also enables us to establish the connection with the MongoDB server.
Mongoose internally make use of the MongoDB driver that we had used in the previous exercise.
So, Mongoose depends upon the MongoDB driver,
so, which means that from your Mongoose-based node application, you can use all the methods that are already available from the MongoDB driver also if you choose to,
but Mongoose itself has its own collection of methods that we can make use of to interact with the MongoDB database.

Exercise (Instructions): Mongoose ODM Part 1
Objectives and Outcomes
In this exercise you will explore the Mongoose ODM and learn about creating schemas and interacting with the MongoDB database using Mongoose methods. At the end of this exercise, you will be able to:

Install Mongoose ODM and connect to a MongoDB Server
Create Mongoose Schemas
Perform Database operations with Mongoose methods

Installing Mongoose:
1.Create a folder named node-mongoose and move into the folder.
2.At the prompt, type the following to initialize a package.json file in the node-mongoose folder:
  npm init
3.Accept the standard defaults suggested
4.n this folder, install Mongoose by typing the following at the prompt:
  npm install mongoose@5.1.7 --save

Implementing a Node Application
1.Create a sub-folder named models in the node-mongoose folder. Move to this folder.
2.Create a file named dishes.js and add the following code to create a Mongoose schema
3.Move to the node-mongoose folder and create a file named index.js and add the code:
4.Make sure that your MongoDB server is up and running. Then at the terminal prompt type the following to start the server and see the result:
  npm start
5.Create a .gitignore file with the contents "node_modules"
6.Initialize the Git repository and do a Git commit with the message "Mongoose Part 1".

Exercise (Instructions): Mongoose ODM Part 2
Objectives and Outcomes
In this exercise you will continue to explore the Mongoose ODM and learn about creating schemas and interacting with the MongoDB database using Mongoose methods. At the end of this exercise, you will be able to:
Perform Database operations with Mongoose methods
Mongoose Operations
1.Now, update index.js
2.Run this server on the console and see the result.
3.Do a Git commit with the message "Mongoose Part 2".

Adding Sub-documents to a Document
1.Update dishes.js in the models folder
2.Update index.js
3.Run the server and observe the result.
4.Do a Git commit with the message "Mongoose Part 3".


Exercise (Instructions): REST API with Express, MongoDB and Mongoose Part 1:

Objectives and Outcomes:
In this exercise, you will integrate the REST API server based on the Express framework that you implemented earlier, together with the Mongoose schema and models to create a full-fledged REST API server. At the end of this exercise, you will be able to:
1.Develop a full-fledged REST API server with Express, MongoDB and Mongoose
2.Serve up various REST API end points together with interaction with the MongoDB server.

Update the Express Application:
1.Go to the conFusionServer folder where you had developed the REST API server using Express generator.
2.Copy the models folder from the node-mongoose folder to the conFusionServer folder.
3.Then install bluebird, mongoose and mongoose-currency Node modules by typing the following at the prompt:
  npm install mongoose@5.1.7 mongoose-currency@0.2.0 --save
  The Mongoose currency adds in support for currency in our schema
  Since our dish is going to contain a price, that is why we are going to be using the Mongoose currency module here
4.Open app.js file and add in the code to connect to the MongoDB server
5.Next open dishes.js in the models folder and update it
6.Now open dishRouter.js and update its code
7.Save the changes and start the server. Make sure your MongoDB server is up and running.
8.You can now fire up postman and then perform several operations on the REST API. You can use the data for all the dishes provided in the db.json file given above in the Exercise Resources to test your server
9.Do a Git commit with the message "Express REST API with MongoDB and Mongoose Part 1"

Exercise (Instructions): REST API with Express, MongoDB and Mongoose Part 2:
Objectives and Outcomes:
In this exercise, you will continue the integration of the REST API server based on the Express framework that you implemented earlier, together with the Mongoose schema and models to create a full-fledged REST API server.
At the end of this exercise, you will be able to:
1.Add support for accessing and updating comments within the dishes.

Handling Comments:
1.Add the code to dishRouter.js to handle comments:
2.Save the changes and start the server. Make sure your MongoDB server is up and running.
3.You can now fire up postman and then perform several operations on the REST API. You can use the data for all the dishes provided in the db.json file given above in the Exercise Resources to test your server
4.Do a Git commit with the message "Express REST API with MongoDB and Mongoose Part 2".

Assignment 2 Instructions:
In this assignment, you will continue your journey with MongoDB and Mongoose.
You will then create two new schemas for promotions and leadership,
and then extend the Express REST API server to support the /promotions and the /leaders REST API end points.

Step-By-Step Assignment Instructions

Assignment Overview:
At the end of this assignment you would have completed the following three tasks:
1.Implemented the Promotions schema and model
2.Implement a REST API to support the /promotions endpoint, and the /promotions/:promoId endpoint enabling the interaction with the MongoDB database
3.Implemented the Leaders schema and model
4.Implement a REST API to support the /leaders endpoint, and the /leaders/:leaderId endpoint enabling the interaction with the MongoDB database

Assignment Requirements:
This assignment consists of the following two tasks:
You are given the following example of a promotion document. You will now create the Promotions schema and model to support the document:
{
      "name": "Weekend Grand Buffet",
      "image": "images/buffet.png",
      "label": "New",
      "price": "19.99",
      "description": "Featuring . . .",
      "featured": false
}
Note in particular that the label and price fields should be implemented the same way as you did for the Dishes schema and model. The Promotions schema and model should be defined in a file named promotions.js.
Next, extend the promoRouter.js to enable the interaction with the MongoDB database to fetch, insert, update and delete information.

Task 2:
You are given the following example of a leadership document. You will now create the Leaders schema and model to support the document:

{
      "name": "Peter Pan",
      "image": "images/alberto.png",
      "designation": "Chief Epicurious Officer",
      "abbr": "CEO",
      "description": "Our CEO, Peter, . . .",
      "featured": false
}

The Leaders schema and model should be defined in a file named leaders.js.
Next, extend the leaderRouter.js to enable the interaction with the MongoDB database to fetch, insert, update and delete information.

Authentication:
1.Basic Authentication is done with username and password
  In basic authentication the client will have to explicitly keep adding in the authorization field containing the username and password for every request that the client sends to the server side.
2.Cookies are yet another mechanism that enables your server to be able to expect the client to store some information on the client side and include that information explicitly in each outgoing request.
  HTTP cookies, are small piece of data that is sent from a web server and is stored on the client side.
  Now, almost all browsers have the ability to support the storing of cookies on the client side, and automatically including them in the request when the request is sent to a specific server.
  So each subsequent request from the client side will include a new header in there with the cookie in the request header.
  So instead of including your base-64 encoded username and password like we did in the basic authentication, using cookies,
  your server may set up an explicit piece of information on the client side which then will be included in each outgoing request from the client side.
3.Now expanding this further, if your server wants to track information about your client, then the server may set up explicitly a session tracking mechanism.


Exercise (Instructions): Basic Authentication

Objectives and Outcomes
In this exercise you will use basic authentication approach to authenticate users.
At the end of this exercise, you will be able to:
1.Set up an Express server to handle basic authentication
2.Use the basic access authentication approach to do basic authentication.

Setting up Basic Authentication:
You will continue with the Express REST API server that you have been working on in the previous module in the conFusionServer folder.
1.Open the app.js file and update its contents
2.Save the changes and start the server. Access the server from a browser by opening an incognito window and see the result.
3.Do a Git commit with the message "Basic Authentication".

Conclusion:
In this exercise you learnt about performing basic authentication with username and password in a browser.

Exercise (Instructions): Using Cookies
Objectives and Outcomes
In this exercise you will examine the use of cookies for authentication.
The server will send a signed cookie to the client upon successful authentication, and expects the client to include the cookie with every subsequent request.
At the end of this exercise, you will be able to:

1.Set up your Express application to send signed cookies.
2.Set up your Express application to parse the cookies in the header of the incoming request messages

Using cookie-parser:
1.The cookie-parser Express middleware is already included in the Express REST API application.
  If you need to add Cookie parser middleware then you can install the NPM module as follows:
  npm install cookie-parser@1.4.3 --save
2.Update app.js
3.Save the changes, run the server and test the behavior.
4.Do a Git commit with the message "Authentication with Signed Cookies".

Conclusions
In this exercise you examined the use of cookies for tracking authenticated users
so that subsequent access to the server can be enabled without need for authentication.

Exercise (Instructions): Express Sessions Part 1:

In this exercise you will use Express sessions to track authenticated users so as to enable authenticated access to server resources. At the end of this exercise you will be able to:
1.Set up your Express server to use Express sessions to track authenticated users
2.Enable clients to access secure resources on the server after authentication.

Installing express-session:
1. Still in the conFusionServer folder, install express-session and session-file-store Node modules as follows:
   npm install express-session@1.15.6 session-file-store@1.2.0 --save
Using express-session:
2. Then, update app.js to use Express session:
3. Save the changes, run the server and examine the behavior.
4. Do a Git commit with the message "Express Session Part 1".

Conclusions:
In this exercise, we have seen how we can set up our express application to use sessions middleware.
And we also saw how we are using the file store middleware to keep track of our sessions.
And then set up our application to use express-sessions rather than using their signed cookies for user authentication.

Exercise (Instructions): Express Sessions Part 2:
In this exercise, we will look at a way of extending our express REST API server to support a new model for registering new users,
authenticating existing users and logging out a user from our server site.
So, we will introduce a new user model and schema into our application.
We'll also look at Express Sessions as a way of tracking the users once the user logs in, and then when the user logs out, the session will be removed from the system
Objectives and Outcomes
In this exercise you will continue to use Express sessions to track authenticated users so as to enable authenticated access to server resources. At the end of this exercise you will be able to:
1.Set up a method for users to register.
2.Authenticate registered users to enable them to access secure resources.
3.Enable clients to access secure resources on the server after authentication.

User Model and User Authentication:
1.Add a new Mongoose model for users in the file named user.js in the models folder.
2.Update users.js in the routes folder to support user registration, login and logout.
3.Save the changes and test the server.
4.Do a Git commit with the message "Express Session Part 2".

Conclusions
In this exercise we built the support for user registration and authentication.
1.We extended our Express server to allow users to register themselves and then allow the user to login to the system, and then also logout from the system.
2.On the server side we are tracking a logged in user using the session and using the cookie on the client side and when the user logs out the cookie is destroyed on the client side.

Passport:
In the previous lessons, we have seen various strategies for user authentication.
We started with basic authentication,
and then moved on to cookies and express sessions as a way of authenticating and tracking users.
In this lesson we will look at Passport, a node module that makes authentication quite easy.
Passport is nothing but an authentication middleware which supports various strategies that can be used for user authentication,
including a local strategy like using username and password,
or even third party authentication or using OAuth or OAuth 2.0, like using Facebook, Twitter, or Google+, and so on.
You can use a local strategy for example which is based upon registering users into your system using a username and password,
and then thereafter authenticating them using the username and password.
Passport also supports OpenID based authentication or OAuth or OAuth 2.0 based authentication, as is supported by third party authenticators like Facebook, Twitter, Google+, and so on.
We can also use what is called as JSON web tokens as another way of authentication called token-based authentication.
Also, Passport supports sessions.
As we have seen in the previous exercise and the previous lesson, express sessions are a easy way of tracking users on the server side and being able to service incoming requests from clients.

The use of Passport within our application is fairly straight forward:
1.On the routes on which we want to perform authentication, we just specify passport.authenticate method and then specify the specific authentication strategy that we want to use for the user authentication.
2.If the authentication is successful, then the middleware moves on to the next step, where we can further process the incoming request.
3.Upon completion of the successful authentication of the user passport, Passport itself adds a user property to the request message.
So req.user becomes available for us with the user's information in there, which we can subsequently use within our express application to handle the request coming from specific users
4.Passport-Local module supports a strategy called as the local strategy for authenticating users with the standard username password combination.
5.We set up the user schema,and use this schema or model to track the username and password
and then we depend upon Passport-Local module to verify the username and password.
6.Passport-Local-mongoose module or plugin adds in the username and a encrypted way of storing the password within our user model.
7.The encryption is done by using hashing on the password that we use for registering users, and the hash itself uses a salt field.
8.The salt is a random string that is used for performing the hashing operation on the password for storing.
  So the hashed password is itself actually stored in our MongoDB database.
9.Also if we are using sessions that are supported by Passport, then, for supporting sessions,
  the user information needs to be serialized to be stored with the session information on the server side
  and then, when the request comes in, from the session ID, the user information needs to be deserialized to extract the user information from our session information that is stored on the server side.

Exercise (Instructions): User Authentication with Passport:

Objectives and Outcomes
In this exercise, you will explore user authentication with the Passport module.
You will be able to control access to your REST API server.
At the end of this exercise, you will be able to:
1.Use Passport module together with passport-local and passport-local-mongoose for setting up local authentication within your server.
2.Use Passport together with sessions to set up user authentication.

1.Installing Passport:
  You will continue working with the Express REST API Server in the conFusionServer folder. You will modify this project to set up user authentication support using Passport.
  Install the Passport and related Node modules:
  npm install passport@0.4.0 passport-local@1.0.0 passport-local-mongoose@5.0.1 --save

2.Updating User Schema and Model:
  In the models folder, update user.js by adding the code to it

3.Adding Passport-based Authentication
  Add a new file named authenticate.js to the project folder and initialize it
  Open users.js file in the routes folder and update the code
  Finally, update app.js.
  Save the changes and test the server by sending various requests.
  Do a Git commit with the message "Authentication with Passport".

Conclusions
In this exercise you used the Passport module to verify the authenticity of users and control access to routes in a REST API server.

Token Based Authentication:
In the previous lessons, we have seen several different kinds of authentication schemes.
We started with basic authentication, then we looked at how we can use cookies for doing authentication,
and even signed cookies, and thereafter, we looked at session-based authentication,
where the server is keeping track of information about each client, and then the cookie will be used as a way of indexing into the server-side database to extract additional information, to validate the user.
The cookie and session-based authentications are not scalable, because the server needs to keep track of all the different users.
So this is where token-based authentication has proved to be very useful.
With cookie based authentication, we notice that cookies are stored on the client side, and the cookies are included in every outgoing request message whereby, the server is reminded about that specific client, by extracting information from the cookie.
Cookie can be used together with sessions, whereby the cookies store the session ID, and then when the server receives the incoming request from the cookie, it extracts the session ID and uses that as an index into the server-side session store to retrieve the session information for the particular client.
Now, this approach as I said, is not very scalable because if you have thousands of sessions, the server needs to keep track of all these thousands of sessions on the server side.
Even though it is done independent of HTTP in a store, either a file store or a database. But still, the fact that you need to track all of these information makes it not scalable.
Session-based authentication, as we have seen earlier, works perfectly fine for web applications and can easily take care of user authentication.
But then, session-based authentication, while violates the principle of stateless servers and also leads to scalability problems.
The second issue is, mobile applications do not handled session-based authentications very well. Similarly, mobile applications have a hard time dealing with cookies.
So, in such circumstances where your server is serving data for both a web application, as well as a mobile app, then, the session based authentication will not be very useful,
and this is where token based authentication becomes a lot more easy to use.
In a token-based authentication as the name implies, the server will issue a token to a validated user, and all subsequent requests coming from the client side, will bear the token in the request itself, either in the form of a request header or in the body of the request message.
Furthermore, token-based authentication also helps us to deal with what are called CORS  Cross-origin resource sharing problems.
Token-based authentication addresses some of the issues that lies with cors and cross-site request forgery related issues.
Not only that, token-based authentication is a lot more easy for one application to share its authentication with another application, of course, this is all done in a secure manner.
But, with session based authentication, that is not straight forward.
In token-based authentication, the user first needs to validate himself or herself on the server side.
Now, this validation could take on the forms that we have seen earlier.
So we can use a local validation using username and password. Or, we can even use third party validation using technologies like, oauth or oauth 2.0 or open ID.
No matter which way the user authenticates, once the user is authenticated, right after, your server can simply issue a token to the user.
And all subsequent communication between the user and the server, can be done simply using this token.
JSON Web Token that we will talk about, is one such token-based authentication scheme,
and there server when it creates this token, it will create a signed token, using a secret on the server site which only the server knows.
So thereby, even if a third party in towards and between and tries to manipulate the token, even if it captures the token and tries to manipulate the token, the token will become invalid.
And so, that way of protecting the user is easily feasible, all subsequent requests from the client side should carry the token in the request, either, in the header or in the body of the request message.
So when the server receives this token, the server will verify the token, to ensure that this is a valid token,
and then if it is a valid token, the server will then respond to the incoming request.
As I mentioned, JSON Web Tokens is one such token-based authentication scheme.
JSON Web Token, is a very simple way of encoding information in a token then pass it to the client site.
JSON Web Token itself is based on standards, this is based on the IETF RFC 7519. IETF stands for the Internet Engineering Task Force.
The organization that mandates everything about how the internet works, and deals with the protocols and the policies, related to the internet.
The RFC, stands for the standards document, in IETF terms, RFC stands for Request for Comments.
And each such standards document carries a number. 7519 in this case refers to the document, the standard document related to JSON Web Token.
The JSON Web Token itself is a self contained token, it carries all the information within itself, that is necessary to identify the user.
Not only that, a JSON Web Token can be shared between two applications.
So for example, one application when it authenticates and then gets hold of a JSON Web Token, can pass that JSON Web Token to and in that application, that it is willing to authorize to access the server on its behalf.
This sharing of the token is done in a very secure manner
Thereby, the authorization is transferred over to a second application, and the second application can authorize on behalf of the first application, to communicate with the server.
This is feasible with tokens
A JSON Web Token, as I said, is encoded into a long string and this string itself can be interpreted as consisting of three parts.
The string itself can, or the encoded string itself contains three parts, the Header, the Payload, and the Signature.
That carries enough information about how this token is encoded.
The Header itself contains the specific algorithm that is used for encoding this JSON Web Token, and the type of the token itself.
The algorithm in this case, would be HS256 which is a 256 bit encoding scheme, that is used for hashing the information inside of the token.
And in this case, this happens to be the JSON Web Token, and so the type field will be set to JWT.
And so that is the information that is stored in the header of JSON Web Token.
The Payload itself, carries information that helps you to identify the user.
In the exercise that we will do, our payload only carry the ID of the user inside the payload. No other information is necessary.
This ID can be used on the server side, to index into the Mongo DB to retrieve the full user information if required.
So, you will see that we'll encoding the ID and then storing it in the payload of that message.
You can store additional information in the payload of the message if you require.
But the more information that you stored there, the larger the corresponding JSON Web Token is going to me.
So, try to limit the amount of information that you stored in the payload of the JSON Web Token.
As we will see in the exercise, we have a node module that enables us to encode and create a JSON Web Token, based on the information that we want to put in the payload.
Now, when you create a JSON Web Token, you also supply a signature.
A secret key on the server side which is used for encoding this JSON Web Token, and that secret is also included in the signature part of the JSON Web Token.
The signature itself is included in such a way that there is a basics before encoded header and payload, which is then encoded using the specific secret that is used by the server.
And this encoded in, as I said the HMAC, that we have referred to in one of the previous lessons and using the 256 bit hashing, and that is included in the signature.
So, when this JSON Web Token is received on the server side, and when the server decodes this token,
then the server is able to cross check to make sure that this JSON Web Token has not been tampered by anybody, while the token is being passed between the client and the server site.
As I mentioned, if you need to deal with JSON Web Tokens in your node application, there is a specific node module called as the jsonwebtoken Node Module.
This node module implements the JSON Web Token related standards, and it can be included into your node application.
This module itself, provides a method called sign which allows you to sign and issue the token to the client from the server side.
It also contains a verify method which can be used to verify the authenticity or to ensure the authenticity of the incoming JSON Web token,
so we will be making use of the JSON Web token module
Together with the JSON Web Token module, we also used the Passport-JWT module, node module, which provides the jwt based strategies for our passport authentication module.
So this provides a passport strategy for authenticating using JSON Web Token.
So this allows you to authenticate RESTful endpoint using the JWT as the method for dong the validation, without requiring the server to use sessions.
Now, the JWT passport module, supports a method of even extracting the JWT token from the incoming request message, and then even verifying the token on your behalf.
The Passport-JWT module in turn uses the JSON Web Token module for doing the verification of that JSON Web Token.
The token itself can be carried in the header of the incoming request, even in the authentication header of the incoming request, which is what we will be doing in the exercise.
The token can be also carried in the body of the incoming request, in which case, we have to extract the token from the body of the incoming request and then make use of it.
The Passport-JWT module, supports that also if you choose to use that as way of passing the token back from the client to the server site.
The JSON Web Token, can be also included in the URL query parameters if you so choose to, and can be extracted from there b Passport-JWT and used for authentication.

Exercise (Instructions): User Authentication with Passport and JSON Web Token:
Objectives and Outcomes
In this exercise, you will explore user authentication with JSON web tokens and the Passport module. You will be able to control access to some routes within your REST server. At the end of this exercise, you will be able to:
1.Use JSON web tokens for token-based user authentication
2.Use Passport module together with passport-local and passport-local-mongoose for setting up local authentication within your server

1.Installing passport-jwt and jsonwebtoken Node Modules:
  You will continue working with the Express REST API Server in the conFusionServer folder.
  You will modify this project to set up user authentication support using tokens and Passport.
  Install the passport-jwt and the jsonwebtoken modules as follows:
  npm install passport-jwt@4.0.0 jsonwebtoken@8.3.0 --save
2.Updating the App to use JSON Web Tokens:
  Create a new file named config.js and add the following code to it
3.Supporting JSON Web Tokens and Verification:
  Update authenticate.js.
  Open users.js file in the routes folder and update the code.
  Update app.js to remove the auth function and the app.use(auth), and update it.
4.Controlling Routes with Authentication:
  Open dishRouter.js and updated the code for the '/' route
  Do similar updates to promoRouter.js and leaderRouter.js.
  Save the changes and test the server by sending various requests.
  Do a Git commit with the message "Passport JWT".
Conclusions
In this exercise you used token-based verification together with the Passport module to verify the authenticity of users and control access to routes in a REST API server.

Mongoose Population:
In this lesson you will learn about how you can reference one document from another in MongoDB. In addition you will use Mongoose population to populate information in a document from a referenced document. At the end of this lesson, you will be able to:
1.Reference a MongoDB document from another document
2.Use Mongoose population to populate information into a document from a referenced document

Document databases, the NoSQL databases, are not designed with relations in mind.
Everything that you need in a document is stored completely within the document. i.e documents are self-contained.
So you do not have support for relations that you might be more familiar with from the relational database world.
MongoDB has taken a few steps in that direction, even with the NoSQL databases.
Now of course, there are situations where you have other documents that already contained the information.
And you may want to pull that information into your existing document, rather than duplicating that information.
MongoDb or Mongoose allow you to store references to other documents within a current document by using ObjectId of the other document ( i.e {type: mongoose.Schema.Types.ObjectId, ref: 'User'} i.e ObjectId referencing to User Schema)
It allows you to perform a way of taking the information from the other document and then enclosing it inside the correct document using the Mongoose population support.
Population is the process of automaticaaly replacing specified paths within a document with documents from another collection or document.
And the compound document will be constructed and then sent back to you.
e.g Dishes.find({}).populate('comments.author')
Now the expectation is that the comments.author field is actually an OjectId which references to the user document.
var commentsSchema  = new Schema({
  author: {
    type: mongoose.Schema.Types.ObjectId,
    ref: 'User'
  }
})
So this populate call that we perform here will then go and fetch from the database each individual author's record or the user's record.
And then take that user document and populates it into the dishes document to construct the compound document from here.
But of course, let me caution you that this population operation is not an easy task for the server to do.
Because every single dish, you will have to examine each and every comment.
Then for each and every comment, then you need to find out their ObjectId for the user.
Then you go and fetch that user document and then populate it inside the dish document.
And then, that has to be repeated for every single comment that is contained in that Dishes document.
It essentially means that it'll take a much longer time for the server side to complete the request and send back the information to the client side.
So I would suggest that you should use populate very judiciously. You should use it only in circumstances where you really need that information.
In our app when we authenticate the user, the user's information is already loaded into every request that comes in from the client side.
And so the user information is available to us.
So when we are posting the comment on the server side, we will also capture the user's ID and then store it in the author field of the comment schema.
So on the server side we should validate the user and only for users that are signed in, we would allow them to post comments.
And then when they post comments, we will automatically fill in the author field for that comment document by substituting the author field with the ObjectId of the user.

Exercise (Instructions): Mongoose Population
In this exercise, we have seen the use of mongoose population and we have also seen how we can populate information from one document into another document.
Whereby, when we modify the server to do the population for the requests, mongoose will automatically take care of populating this information for us.
All that we need to do, is to store the reference to the other document in the form of the object ID, in the document into which you want to populate this information.
Objectives and Outcomes
In this exercise you will learn about how you can reference one document from another in MongoDB. In addition you will use Mongoose population to populate information in a document from a referenced document. At the end of this exercise, you will be able to:
1.Use Mongoose population to cross-reference users within a comment
2.Populate the comments with the users information upon querying

Using Mongoose Population:
1.Open user.js and update the code for the schema.
2.Open dishes.js and update the comment schema
3.Open dishRouter.js and update the routers and the methods.
4.Open users.js and update the register function
5.Save all the changes and start the server and test.
6.If you happen to have a registered user in your MongoDB, then use the Mongo REPL to delete the user.
7.Do a Git commit with the message "Mongoose Population".

Conclusions
In this exercise you learnt about cross-referencing Mongo documents using the ObjectId.
In addition you learnt about using Mongoose population support for populating documents.

Assignment 3: User Authentication
In this assignment you will continue the exploration of user authentication.
We have already set up the REST API server to validate an ordinary user.
Now, you will extend this to verify an Admin and grant appropriate privileges to an Admin.
In addition you will allow only a registered user to update and delete his/her submitted comments.
Neither another user, nor an Admin can edit these comments.

Step-By-Step Assignment Instructions
less
Assignment Overview

At the end of this assignment, you would have completed the following:

1.Check if a verified ordinary user also has Admin privileges.
2.Allow any one to perform GET operations
3.Allow only an Admin to perform POST, PUT and DELETE operations
4.Allow an Admin to be able to GET all the registered users' information from the database
5.Allow a registered user to submit comments (already completed), update a submitted comment and delete a submitted comment.
  The user should be restricted to perform such operations only on his/her own comments.
  No user or even the Admin can edit or delete the comments submitted by other users.

Assignment Requirements:

This assignment is divided into three tasks as detailed below:

Task 1

In this task you will implement a new function named verifyAdmin() in authenticate.js file.
This function will check an ordinary user to see if s/he has Admin privileges.
In order to perform this check, note that all users have an additional field stored in their records named admin, that is a boolean flag, set to false by default.
Furthermore, when the user's token is checked in verifyOrdinaryUser() function, it will load a new property named user to the request object.
This will be available to you if the verifyAdmin() follows verifyUser() in the middleware order in Express.
From this req object, you can obtain the admin flag of the user's information by using the following expression:
req.user.admin
You can use this to decide if the user is an administrator.
The verifyAdmin() function will call next(); if the user is an Admin, otherwise it will return next(err);
If an ordinary user performs this operation, you should return an error by calling next(err) with the status of 403, and a message "You are not authorized to perform this operation!".

Task2

In this task you will update all the routes in the REST API to ensure that only the Admins can perform POST, PUT and DELETE operations. Update the code for all the routers to support this. These operations should be supported for the following end points:

1.POST, PUT and DELETE operations on /dishes and /dishes/:dishId
2.DELETE operation on /dishes/:dishId/comments
3.POST, PUT and DELETE operations on /promotions and /promotions/:promoId
4.POST, PUT and DELETE operations on /leaders and /leaders/:leaderId

Task 3

In this task you will now activate the /users REST API end point.
When an Admin sends a GET request to http://localhost:3000/users you will return the details of all the users.
Ordinary users are forbidden from performing this operation.

Task 4

In this task you will allow a registered user to update or delete his/her own comment.
Recall that the comment already stores the author's ID.
When a user performs a PUT or DELETE operation on the /dishes/:dishId/comments/:commentId REST API end point, you will check to ensure that the user performing the operation is the same as the user that submitted the comment.
You will allow the operation to be performed only if the user's ID matches the id of the comment's author.
Note that the User's ID is available from the req.user property of the req object.
Also ObjectIDs behave like Strings, and hence when comparing two ObjectIDs, you should use the Id1.equals(id2) syntax.

HTTPS and Secure Communication:
Zny time we need to use user authentication, first, in the form of, say, supplying the credentials to authenticate yourself
and thereafter, authenticating yourself by including either the cookie or the token in the header of the request message,
we should be doing this over a secure channel.
which means that we should not be using HTTP and then including these credentials into either the header or the body of the request messages without encryption.
Check the slides for how HTTPS works.
Cryptography:
If you need to send a message over a channel to another user, then you would want to be able to encrypt the message in such a way that only the receiver will be able to decrypt the message
and obtain the information that you are trying to send to the receiver.
So both the sender and the receiver should understand on establish between the two of them how this encryption process and how the decryption is going to work.
For this to work, any encryption and decryption works based on exchange of secret keys.
So in symmetric key cryptography, both the sender and the receiver will share a secret key between the two sites,
and this secret key is known only to the sender and to the receiver side.
So when the sender needs to send the message,
then the sender will encrypt this message using a cryptography algorithm, which uses the secret key as the other input to this algorithm.
And once this message is passed through this cryptography algorithm, then an encrypted message will be generated.
Now, this encrypted message will be sent to the channel across to the receiver side.
If you have a third party malicious user sitting in between and listening and capturing this encrypted message,
they would have a hard time decrypting this message because for decrypting an encrypted message, you still need the secret key.
Now, on the receiver side, when the encrypted message is received, then the receiver will apply and a decryption algorithm,
which also takes as the input, the same secret key that was used on the sender side to encrypt the message.
So upon decryption, the original message will be retrieved and can be processed on the receiver side.
Now, if a malicious third party wants to decrypt this encrypted message, they are going to face an uphill battle because the process of encrypting using the secret key will turn the message and can, in turn, encrypt the message.
Without possessing the secret key, it is going to be next to impossible to decrypt the encrypted message.
If you use brute force techniques, then eventually, you will end up decrypting the encrypted message.
But it is going to take so long and require so much of computation power. It will be not worth the effort for the third party malicious user to try and decode the encrypted message.
So you are essentially making it really difficult for somebody to decrypt the message if they don't possess the secret key.
Now, since the secret key is known only to the sender and the receiver, the two end parties, the sender and the receiver, can communicate with the assurance that in encrypted message from the sender side can only be decrypted by the receiver side.
So this is how symmetric key cryptography works.
The fact that you have the same secret key being shared between the sender and the receiver means that the inclusion of decryption process used the same secret key, hence, symmetric key cryptography.
Of course, with the symmetric key cryptography, one of the issues is that both the sender and the receiver needs to have access to the same secret key.
Now, if the sender and receiver are communicating over an insecure channel, it is going to be difficult for both sides to come to an understanding about the same secret key without disclosing it to others.
So this is where another algorithm called public-key cryptography is very useful.
In public-key cryptography, the idea is that you have two different keys.
You have a public key and a private key.
Now, the public key can be widely distributed to anyone that you want.
So when somebody wants to send a message to you, they are going to use your public key to encrypt the message.
So if a sender here wants to send the message to the receiver, then the sender will use the public key off the receiver to encrypt the message on the sender side using the encryption algorithm.
Now, once the encrypted message is sent across over the insecure channel to the receiver side, the receiver will then use the private key that only the receiver knows in order to decrypt.
Now, for public-key cryptography to work as we saw, the public key can be widely distributed without any concern.
But since the private key is only known to the receiver side, only the receiver will be able to decrypt the message,
and, again, a third-party intruder that captures this encrypted message will find it inordinately difficult to decrypt the message.
Now, of course in public-key cryptography, the public and the private key are two different keys.
Now, then your obvious next question would be why not simply use public-key cryptography for encryption.
The problem is that using public-key cryptography for encryption and decryption is an expensive process, and so that is why we don't use public-key cryptography for their entire communication.
Instead, the public-key cryptography is going to be used primarily for the sender and the receiver to agree upon the common secret key that the two are going to use.
We will later on see how the public-key cryptography can be used to establish the common secret key between the sender and the receiver and then subsequently use symmetric key cryptography for further communication.
One protocol that uses this approach is the Secure Sockets Layer and also the Transport Layer security protocols, SSL and TLS in short.
These cryptography protocols enable secure communication between the sender and the receiver over an insecure network like the Internet.
The sender and the receiver will communicate over this Internet using encrypted messages, which only the sender and the receiver can decrypt.
And this approach, either the SSL or TLS, uses a combination of public-key cryptography together with symmetric key cryptography.
Their exact process of doing this, is explained in the slide.
In addition, using SSL or TLS, we are trying to maintain two different things.
We are, first, trying to maintain the privacy of the communication between the sender and the receiver so that no malicious third party can extract the message from the encrypted message.
Second, we are also trying to maintain integrity, meaning that when the sender sends a message, the receiver will be able to be assured that the message has not being tampered with.
So both security and maintaining integrity is very important in this case.
So both privacy and integrity has to be maintained by this secure communication protocol that we build for exchange between the sender and the receiver.
Let's briefly talk about how SSL or TLS actually work.
This is done through a handshaking process which has been illustrated in the slide.
When a client wants to communicate with the server, the client sends a message to the server, specifying that the client wants to communicate with the server securely.
At this point, the server will send back the certificate to the client, which contains a public key, which has been certified by the certification authority as belonging to that particular server.
So that way, when the client receives this public key plus the certification by the certification authority, the client will be able to verify the server's credentials.
So, the client needs to establish that it is really talking to the server, that it is intending to communicate with.
So at this point, when the client verifies the server's credentials, the client now has access to the public key of the server.
Once the client gets hold of the public key of the server, then the client will generate what is called as the pre-master secret.
This pre-master secret is something that both the client and server will use to generate what is called as a session key.
So, the client generates a pre-master secret, the client then encrypts that secret using the server's public key, and then sends the secret across to the server.
Now, remember that once the secret is encrypted using the public key, nobody other than the server will be able to extract the information from the encrypted message.
So, when the server receives this encrypted message, the server extracts the pre-master secret from this message.
Now, both the client and the server possess the same pre-master secret.
At this point, both the client and the server will use the same set of steps starting with the pre-master's secret, and with the same set of values, that will generate a key called as the session key.
Now, when the session key is generated both on the clients side and the server side, it will be exactly the same session key, because both will follow exactly the same process for generating the session key.
So, at this point, both the client and the server, now possess a secret key which is the same on both the sites.
So, all subsequent communication between the server and the client, can then proceed using symmetric encryption.
So, when the client needs to communicate with the server, the client will encrypt the data using the secret session key, and then send over their data.
Similarly, when the server needs to communicate with the client, the server will obviously use the same session key to encrypt the data and then send it over to the client.
Now, since the client possesses the same session key, the client will be able to decrypt the message and extract the unencrypted message.
By using this procedure, the client and the server has ensured that the communication between them is private.
Also, we manage to ensure that no malicious third party can intercept the message and cause any changes to the message.
So, integrity of the message is also maintained, and the privacy of the communication between the client and the server is also maintained.
Now, if you are interested in learning more about this, one good source for learning about Cryptography Protocols is a very good book by Jim Crozon Keith Ross called Computer Networks.
This book has a very easy to understand chapter about cryptography and security as applied to network communication.
Now that we have established the process for being able to communicate securely between a client and server,
let's look at how the internet itself leverages this, for communication between a client and the server using HTTP.
Now, this is where the HTTPS protocol comes into the picture.
As you already understand about the internet, the internet is a layered architecture, where the IP and the TCP form the network, and the transport layer which runs on top of the underlying network.
Now, on top of the transport layer, you have the secure socket layer or the transport layer security lining as a thin layer on top of TCP which ensures secure communication between the client and the server.
And on top of it HTTP can run. So, HTTP essentially involves HTTP plus the use of encryption, decryption supported through SSL and TLS.
From the perspective of the implementation of the server side, what we have understood here is sufficient enough for us to understand how to configure a server to use secure communication between the client and the server.
Now, of course the first question that will come to your mind is, that the server needs a public key and a private key.
For a public key cryptography, how does the server generate this?
If you are running a real production server in your environment and providing service for users to access your server, then obviously you need to go through the certification process.
This is where you will approach a certification authority, for example, corporations like VeriSign and Thawte Corporation which are Public Certification Authorities.
There are a few more around the world.
So, you will approach them, and these Certification Authorities will then authenticate your credentials, they will ensure that you are who you claim to be, and then they will verify your credentials, and then at that point, they will issue you a public key and a private key for use on your server site.
So once they issue the public key and the private key, the public key will be certified by the Certification Authority, and then the public key will also carry, in addition, the certificate.
So, this is the certificate that you will send to the client's site.
Since clients can establish the authenticity of these Certification Authorities, if you look at any browser you would notice that most browsers will have the certificates for all these established Certification Authorities already built into them.
So they will be able to establish your credentials, or rather, they will be able to establish that the private key belongs to you, by obtaining your certificate and then checking or verifying your certificate knowing that it has been signed by one of these established Certification Authorities.
Upon this process, client will be able to establish your authenticity.
Now, in this course, we just want to understand how each HTTPS works, and also want to have a simple way of setting up the server with a public key and the private key.
Since we are doing this as an exercise to understanding HTTPS, we can use a tool called open SSL that is already available on our computers to generate what is called the self-signed certificate.
Self-signed keys are not acceptable in the outside work.
But since we know that we are using it only for our testing purposes, we can use self-signed certificates, simply for understanding the process of the secure communication between the client and server.
So, how do we use open SSL? So far using open SSL you can generate Keys, using three commands that I show you here.
You execute these three commands in that sequence, as specified here and that will help you to generate a private key and a certificate that you can make available from the your HTTPS server for clients to download and thereby obtain your public key for secure communication.
So this is the operation we are going to do in our exercise that follows this lecture to establish and issue DPS service.
So the three steps that we do is, first, we will generate the private key by using the first command.
Then after that we will generate a cert.csr which will then be used for us to generate a certificate that can be distributed to the client side by the third command shown there.
So these steps will enable you to generate a private key and also a corresponding certificate that can be issued to the client.

Exercise (Instructions): HTTPS and Secure Communication:
In this exercise you will explore the use of the HTTPS server core node module to create and run a secure server. You will also learn about generating your private key and public certificate and use them to configure your Node HTTPS server. At the end of this exercise, you will be able to:

1.Configure a secure server in Node using the core HTTPS module
2.Generate the private key and public certificate and configure the HTTPS server
3.Redirect traffic from the insecure HTTP server to a secure HTTPS server.

Generating Private Key and Certificate:
1.Go to the bin folder and then create the private key and certificate by typing the following at the prompt:
  openssl genrsa 1024 > private.key
  openssl req -new -key private.key -out cert.csr
  openssl x509 -req -in cert.csr -signkey private.key -out certificate.pem
2.Note for Windows Users:
  If you are using a Windows machine, you may need to install openssl.
  You can find some openssl binary distributions here, https://wiki.openssl.org/index.php/Binaries
  Also, this article gives the steps for generating the certificates in Windows,  https://blog.didierstevens.com/2015/03/30/howto-make-your-own-cert-with-openssl-on-windows/
  Another article provides similar instructions, https://www.faqforge.com/windows/use-openssl-on-windows/
  Here's an online service to generate self-signed certificates,  http://www.selfsignedcertificate.com/

Configuring the HTTPS Server:

1.Open the www file in the bin directory and update its contents
2.Open app.js and add code to the file
3.Run the server and test.
4.Do a Git commit with the message "HTTPS".

Conclusion:
In this exercise, you learnt about configuring a secure server running HTTPS protocol in our Express application.

Uploading Files:
Multer is the node module where that will help the server handle multipart/form-data.
It is an express middleware that enables us to process the incoming request message that contains this multipart/form-data in the request message on server side.
This is written on top of another npm module called busboy.
Busboy is a module that processes incoming HTML form data, general HTML form data.
Multer will upload the incoming form data and it adds a body object to the req.
So you will have a req.body object and also a req.file object.
If you upload a single file, then it'll continue req.file object.
Then if you set up your Multer to accept multiple file uploads, then you can set up the req.files object.
The files object will be an array which contains all the information for each particular file that is uploaded on the server-side.

Exercise (Instructions): Uploading Files
In this exercise you will leverage the Multer module to set up your Express server to accept file uploads. At the end of this exercise you will be able to:

1.Configure the Multer module to enable file uploads
2.Use the Multer module in your Express application to enable file uploads to a designated folder

Installing Multer
1.At the prompt in your conFusionServer project, type the following to install Multer:
  npm install multer@1.3.1 --save
Setting up File Uploading
2.Add a new Express router named uploadRouter.js in the routes folder and add code to it.
3.Then update app.js to import the uploadRouter and configure the new route
4.Save all the changes and test your server.
5.Do a Git commit with the message "File Upload".

Conclusions
In this exercise you learnt to leverage the Multer module to configure your Express server to enable file uploads.


